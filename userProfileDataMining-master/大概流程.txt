1.数据预处理 
	分割特征 Age_dict[ID]=single_query_list.pop(0) 
	过滤停用词  提取每句话的关键词 
			train_key_word =  jieba.analyse.extract_tags(sentence)
				for i  in train_key_word:
                    			    if(i not in stop):
                                                                       key_word_list.append(i)
	然后按照年龄 性别 学历存入文件
	fw_dict_keywords.write('{0}'.format(key))
                fw_dict_keywords.write(' '+(Age_dict[key]))
                fw_dict_keywords.write(' '+Gender_dict[key])
                fw_dict_keywords.write(' '+Education_dict[key]+' ')
                fw_dict_keywords.write(' '.join((value))+'\n')
2.构建训练集数据和测试集数据
        for single_query in train_data:
            # 先将所有的字段分割
            single_query_list = single_query.split(' ')
            # 去除 ID 字段
            single_query_list.pop(0)  # id
                # 删除3个目标变量，剩下关键词
                single_query_list.pop(0)
                single_query_list.pop(0)
                single_query_list.pop(0)
                # 列表转换为字符串，列表中的逗号转换为空格，将字符串所有的单引号去掉，将列表的左中括号和右中括号去掉，最后将换行符去掉
                # 所以最后剩下的是所有关键词构成的字符串，它们分别用逗号分隔
                train_words.append(
                    (str(single_query_list)).replace(',', ' ').replace('\'', '').lstrip('[').rstrip(']').replace('\\n',''))
	最后返回 训练集输入train_words（全关键词）train_tags 训练集目标变量 test_words（全关键词）test_tags 训练集目标变量 
3.特征选择+特征融合
	# 方法二：tv + 卡方选择，tv + LDA，然后进行特征融合，运行时间一个半小时左右
    	train_data, test_data = feature_union_lda_tv(train_words, test_words, train_tags, n_dimensionality, n_topics)
		LDA主题提取
		lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, learning_method='online')
			每种词汇在该训练文本中出现的频率
				tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)
				train_tf = tf_vectorizer.fit_transform(train_words)
	    			test_tf = tf_vectorizer.transform(test_words)	
		# 归一化lda
    		train_data_lda_normalize = preprocessing.normalize(train_data_lda, norm='l2')
    		test_data_lda_normalize = preprocessing.normalize(test_data_lda, norm='l2')
    		# #向量化
    		train_data_tv, test_data_tv = tfidf_vectorize_1(train_words, train_tags, test_words, n_dimensionality)
			# 将训练集数据和测试集数据转换为 tf-idf 特征矩阵，然后使用 chi2 进行特征选择
			tv = TfidfVectorizer(sublinear_tf=True)
			tfidf_train_2 = tv.fit_transform(train_words);  # 得到矩阵
			tfidf_test_2 = tv.transform(test_words)
				# 使用 chi2 方法来选择 n_dimensionality 个最重要的特征
				ch2 = SelectKBest(score_func=chi2, k=n_dimensionality)
    				train_data = ch2.fit_transform(train_data, train_tags)
   				test_data = ch2.transform(test_data)
    		# 特征矩阵合并
    		train_data = bmat([[train_data_lda_normalize, train_data_tv]])
    		test_data = bmat([[test_data_lda_normalize, test_data_tv]])
4.SVM训练数据
	svclf = SVC(kernel='linear')  # default with 'rbf'
    	svclf.fit(train_data, train_tags)
    	pred_tags = svclf.predict(test_data)
	返回预测标签
5.计算正确率
    	evaluate_single(np.asarray(test_tags), test_tags_prediction)
		actual = test_tags
    		pred = test_tags_prediction
    		print('accuracy_score:{0:.3f}'.format(accuracy_score(actual, pred)))
    		print('confusion_matrix:')
    		print(confusion_matrix(actual, pred))






















	